# -*- coding: utf-8 -*-
"""Final_Gods Eye.ipynb

Automatically generated by Colaboratory.

Original file is located at
    https://colab.research.google.com/drive/1F3dEs_4Im3JC2UaRaI14_VirnzwWkzPb

## GOD'S EYE : Mapping Street View Images within the United States

*Dhara Patel and Sparsh Bansal*

*Sept. 2019 - Oct. 2019.*


---

**All code is organized [here](https://github.com/dharaspatel/Gods-Eye/blob/master/godseye_cnn.py) and will be referenced throughout this report for clarity.*

**BACKGROUND**

Humans are exceptional at determining location by using cues like driving direction, vegetation, weather, languages, and other less evident facets. We attempt to train a deep neural network that uses the same human reasoning provided Google street view images to determine what state in the U.S. the picture was taken. 

When we started this project we wanted to make our model applicable to the real world, a useful tool. One way we can make it useful is by training it on images with people in it. If the model can recognize what state that person is in using the same algorithim, it can be a tool for police to track down criminals or find missing persons. 

However, there are several ethical implications to this application. For example, if a company uses this algorithm to create big data on say the age demographics for states and areas. They can control the the product/service offerings based on this data. Another example is violations of the right to privacy - if this software is developed further for higher accuracy, it will be able to track people very easily. An great addition in that respect would be a data mining algorithm that taps into all the cameras in the world through the internet and captures images, uses computer vision to recognize the subject in those images, and run this algorithm to determine their location - completely getting rid of the GPS tracking process which is currently in use. This was the idea used in the Hollywood Film "Furious 7" as "[God's Eye](https://www.youtube.com/watch?v=xlUD7Ui-kv4)".

**PRE-PROCESSING**

Our data is a collection of 100,013 Google Street View images taken in all 50 U.S. states. We downloaded the data from [DeepGeo by Sudharshan Suresh](https://arxiv.org/pdf/1810.03077.pdf). The images are originally 3 x 256 x 256, but were resized to 3 x 128 x 128, reducing the execution time by 4 hours. Figure 1 is a sample image taken in the state of Alabama. Data was organized by state into folders so that every state is class.

Data was then separated into training and test sets. 90 percent of each folder was put into a training set and the other 10 percent was put into a test set.

**BUILDING A NETWORK**

For our first iteration, we chose to explore a convolutional network for our model. We chose a CNN because it is simply enough to implement for the first iteration of our model. Initially all parameters were kept standard and the network only had one layer (see Figure 2).
"""

![Figure 2a](https://github.com/dharaspatel/Gods-Eye/blob/master/Figure2a.jpg)

![Figure 2b](https://github.com/dharaspatel/Gods-Eye/blob/master/figure2b.png)

"""We decided to play around with the number and size of fully connected layers, number and size of kernals, and batch size. None of these changes had a positive impact on our error except the addition of a third fully connected layer. Adding a third *fc* increased the test accuracy from 0.08 to 0.1."""

![Figure 3](https://github.com/dharaspatel/Gods-Eye/blob/master/figure3.png)

"""At this point, we only had a single convolution layer in our CNN. This is a problem because the model is too simple and only low-level features are captured. So, we added two more layers (see Figure 4). We expected the accuracy to increase substantially, but it decreased to 0.06 (See Figure 5)."""

![Figure 4](https://github.com/dharaspatel/Gods-Eye/blob/master/Figure4.jpg)

![Figure 5](https://github.com/dharaspatel/Gods-Eye/blob/master/figure5.png)

"""After several hours of debugging, we realized we hadn't called the activation function after the second convolutional layer. After adding this and decreasing the learning rate to .01, our test accuracy increased to 0.1796. The activation function is important because it implements the sigmoid function and gets rid of linearity in the model. Figure 6 is a graph of training and validation loss over time. The training loss makes several steep drops over time because of automatic reduction in learning rate."""

![Figure 6](https://github.com/dharaspatel/Gods-Eye/blob/master/figure6.png)

"""**RESULTS AND ANALYSIS**

Our final model is a tri-layer convolutional network. It has a train loss of 0.68 and test loss of 0.18. Our accuracies were calculated using *argmax()*, which _. 

One image our model kept mistaking Figure 1. This is a street view image of Alabama, but it kept guessing Arizona. This is interesting because it shows that the model knows Arizona has red sand (https://images.app.goo.gl/cxHfyHHgkTVto7JK6) and Alabama tends to have green grass (https://images.app.goo.gl/vuxL6FvC3fGiNgGG7). 

In general, this model is more accurate than guessing, which is surprising because it only has 3 convolutional layers.

**NEXT STEPS**

The next step is to explore other types of networks: *DenseNet*, *SqueezeNet*, and *Inception*. We also want to analyze what states are more easily recognizable and what states are easy to confuse. 

As of now all the images in our dataset do not include humans. Another next step would be to train the model on outside images with humans in them.
"""

